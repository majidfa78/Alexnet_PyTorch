{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AlexNet Architecture**"
      ],
      "metadata": {
        "id": "xfNPxWCjUpzO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VbkCvU0MUQ5b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the network"
      ],
      "metadata": {
        "id": "PXmddK2uAUf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the AlexNet model\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2),  # Input: 1 channel (grayscale)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # Output: 64 channels\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # Output: 192 channels\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),   # Output: 256 channels\n",
        "            nn.Dropout()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 3 * 3, 4096),  # Adjust input size accordingly\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 10)  # 10 output classes for MNIST\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        # print(\"features\",x.shape)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        # x= nn.Flatten(x)\n",
        "        # print(\"after flatten\",x.shape)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "JO0U0fZCXKvJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nI9p0IvqYAzF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load data"
      ],
      "metadata": {
        "id": "cyeHt7VOAcGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # Resize to fit AlexNet input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_set, val_set = random_split(train_dataset, [0.9, 0.1])\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=2048, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=2048, shuffle=True)"
      ],
      "metadata": {
        "id": "cWqGb8qUYFPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ea57c3-2422-4701-968c-ee2fdd279ddb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:12<00:00, 806kB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 18.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.85MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = AlexNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "GbVht7JPYOf7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for idx,(images, labels) in enumerate(train_loader):#train\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if(idx % 5 == 0):\n",
        "          print(f'batch:{idx} , batch loss:{loss.item()}\\n')\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    acc=0\n",
        "    for idx,(images, labels) in enumerate(val_loader): #validation\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = torch.argmax(model(images),dim=1)\n",
        "        comp=torch.eq(labels,outputs).float().to('cpu')\n",
        "        acc+=torch.sum(comp)/len(comp)\n",
        "        # torch.cat([acc,torch.sum(comp)/len(comp)])\n",
        "\n",
        "    # print(f'batch : {idx} , accuracy : {acc/(idx+1):.4f} %')\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Total loss: {total_loss / len(train_loader):.4f} , Accuracy : {acc/(idx+1)} %')\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCv_xdx8XfXa",
        "outputId": "90a2f87b-a2f4-4774-b21f-954ee922fa21"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch:0 , batch loss:0.021643318235874176\n",
            "\n",
            "batch:5 , batch loss:0.021922262385487556\n",
            "\n",
            "batch:10 , batch loss:0.01290686335414648\n",
            "\n",
            "batch:15 , batch loss:0.02091917209327221\n",
            "\n",
            "batch:20 , batch loss:0.012219661846756935\n",
            "\n",
            "batch:25 , batch loss:0.026842720806598663\n",
            "\n",
            "Epoch [1/5], Total loss: 0.0244 , Accuracy : 0.9877614974975586 %\n",
            "batch:0 , batch loss:0.013479269109666348\n",
            "\n",
            "batch:5 , batch loss:0.021226637065410614\n",
            "\n",
            "batch:10 , batch loss:0.028376800939440727\n",
            "\n",
            "batch:15 , batch loss:0.016954727470874786\n",
            "\n",
            "batch:20 , batch loss:0.0236023161560297\n",
            "\n",
            "batch:25 , batch loss:0.023316070437431335\n",
            "\n",
            "Epoch [2/5], Total loss: 0.0242 , Accuracy : 0.9871719479560852 %\n",
            "batch:0 , batch loss:0.02218485251069069\n",
            "\n",
            "batch:5 , batch loss:0.03618291765451431\n",
            "\n",
            "batch:10 , batch loss:0.014803973957896233\n",
            "\n",
            "batch:15 , batch loss:0.0226740725338459\n",
            "\n",
            "batch:20 , batch loss:0.024812743067741394\n",
            "\n",
            "batch:25 , batch loss:0.018595093861222267\n",
            "\n",
            "Epoch [3/5], Total loss: 0.0198 , Accuracy : 0.9878230094909668 %\n",
            "batch:0 , batch loss:0.01968802697956562\n",
            "\n",
            "batch:5 , batch loss:0.017908329144120216\n",
            "\n",
            "batch:10 , batch loss:0.013319684192538261\n",
            "\n",
            "batch:15 , batch loss:0.019175533205270767\n",
            "\n",
            "batch:20 , batch loss:0.017110832035541534\n",
            "\n",
            "batch:25 , batch loss:0.018160618841648102\n",
            "\n",
            "Epoch [4/5], Total loss: 0.0164 , Accuracy : 0.9910508990287781 %\n",
            "batch:0 , batch loss:0.018014905974268913\n",
            "\n",
            "batch:5 , batch loss:0.01008261926472187\n",
            "\n",
            "batch:10 , batch loss:0.023866012692451477\n",
            "\n",
            "batch:15 , batch loss:0.020865725353360176\n",
            "\n",
            "batch:20 , batch loss:0.018609968945384026\n",
            "\n",
            "batch:25 , batch loss:0.014316431246697903\n",
            "\n",
            "Epoch [5/5], Total loss: 0.0156 , Accuracy : 0.9901509284973145 %\n",
            "Training complete!\n"
          ]
        }
      ]
    }
  ]
}